---
langr: xxxx
langr-audio: 
langr-origin: 
langr-pos:
---

^^^article
The market reaction, when it came, was brutal. On January 27th, as investors realised just how good DeepSeek’s “v3” and “R1” models were, they wiped around a trillion dollars off the market capitalisation of America’s listed tech firms. Nvidia, a chipmaker and the chief shovel-seller of the artificial-intelligence (AI) gold rush, saw its value fall by $600bn. Yet even if the Chinese model-maker’s new releases rattled investors in a handful of firms, they should be a cause for optimism for the world at large. DeepSeek shows how competition and innovation will make ai cheaper and therefore more useful.

DeepSeek’s models are practically as good as those made by Google and OpenAI—and have been produced at a fraction of the cost. Barred by American export controls from using cutting-edge chips, the Chinese firm undertook an efficiency drive, even reprogramming the chips it used to train the model to eke out every drop of power. The cost of building an AI model that can stand toe-to-toe with the best has plummeted. Within days of its release, DeepSeek’s chatbot was the most downloaded app on the iPhone.

The contrast with America’s approach could not be starker. Sam Altman, the boss of OpenAI, has spent years telling investors—and America’s new president—that vast sums of money and computing power are needed to stay at the forefront of AI. Investors have accordingly been betting that a handful of firms stand to reap vast monopoly-like rents. Yet if fast followers such as DeepSeek can eat away at that lead for a fraction of the cost, then those potential profits are at risk.

Nvidia became the most valuable listed company in the world thanks to a widespread belief that building the best AI required paying through the nose for its best chips (on which its profit margins are reported to exceed 90%). No wonder DeepSeek’s success led to a stockmarket drubbing for the chipmaker on January 27th. Others in the data-centre business are also licking their wounds, from Siemens Energy (which would have built the turbines to power the build-out) to Cameco (which would have provided the uranium to fuel the reactors to turn the turbines). Had OpenAI been listed, its stock would surely have taken a tumble as well.

Yet there are far more winners than losers from the DeepSeek drama. Some of them are even within tech. Apple will be cheering that its decision not to throw billions at building AI capabilities has paid off. It can sit back and pick the best models from a newly commoditised selection. Smaller labs, including France’s Mistral and the Emirati TII, will be racing to see if they can adopt the same improvements, and try to catch up with their bigger rivals.

Moreover, efficiency gains are likely to result in greater use of ai. The Jevons paradox—the observation that greater efficiency can lead to more, not less, use of an industrial input—may come into play. The possible applications for a language model with computing costs as cheap as DeepSeek’s ($1 per million tokens) are vastly more numerous than those for Anthropic’s ($15 per million tokens). Many uses for cheaper AI are as yet unimagined.

Even Nvidia may not suffer too much in the long run. Although its market clout may be diminished, it will continue to sell chips in vast quantities. Reasoning models, including DeepSeek’s R1 and OpenAI’s o3, require much more computing power than conventional large language models to answer questions. Nvidia will be hoping to supply some of that.

However, the real winners will be consumers. For AI to transform society, it needs to be cheap, ubiquitous and out of the control of any one country or company. DeepSeek’s success suggests that such a world is imaginable. Take Britain, where Sir Keir Starmer, the prime minister, has unveiled a plan to use AI to boost productivity. If he does not need to pay most of the efficiency gains back to Microsoft in usage fees, his proposal has a better chance of success. When producers’ rents vanish, they remain in the pockets of users.

Some have begun to suggest that DeepSeek’s improvements don’t count because they are a consequence of “distilling” American models’ intelligence into its own software. Even if that were so, r1 remains a ground-breaking innovation. The ease with which DeepSeek found greater efficiency will spur competition. It suggests many more such gains are still to be discovered.

For two years the biggest American ai labs have vied to make ever more marginal improvements in the quality of their models, rather than models that are cheap, fast and good. DeepSeek shows there is a better way.


^^^words
^^^notes